{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNIST \n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "from numpy import linalg as LA\n",
    "from future.utils import iteritems\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics, datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab session you will\n",
    "+ explore some programming concepts in Python, SciKitLearn, and Numpy to implement several statistical / Bayesian classifiers and compare them against each other and with one provided in SciKitLearn,\n",
    "+ get acquainted with another version of the MNIST dataset, and \n",
    "+ explore the effects of specific phenomena in the data on the classification results, also depending on the choice of discrete vs continuous implementation of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Gaussian NB Classifier\n",
    "\n",
    "Make use of the provided Gaussian NB Classifier (sklearn.naive_bayes GaussianNB) for all data sets as a comparison. It is already implemented in the handout for the MNIST_Light set (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Nearest Centroid Classifier (NCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your own Nearest Centroid Classifier (NCC): The NCC fit method should simply compute the mean values over the attribute values of the examples for each class. Prediction is then done by finding the argmin over the distances from the class centroids for each sample. This classifier should be run on all three variants of data sets, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCC:\n",
    "    def __init__(self):\n",
    "        self.mean = {}\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        inidices_labels = defaultdict(list)\n",
    "        for i in range(len(y_train)):\n",
    "            # Collect indices of exemplars for the given class label (keys)\n",
    "            inidices_labels[y_train[i]].append(i)\n",
    "                    \n",
    "        for index, label in enumerate(inidices_labels.keys()):\n",
    "            # subselect an array of exemplars associated with\n",
    "            # this label\n",
    "            matrix_same_class = X_train[inidices_labels[label]]\n",
    "            \n",
    "            # Compute centroid for a set of training vectors X_train, axis=0 since we want columnwise\n",
    "            centroid = matrix_same_class.mean(axis=0)\n",
    "\n",
    "            self.mean[label] = centroid\n",
    "        return None\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for i in X_test:\n",
    "            temp_vec = []\n",
    "            \n",
    "            # Predicting the outcome\n",
    "            for j in self.mean.values():\n",
    "                norm = LA.norm(i-j)\n",
    "                temp_vec.append(norm)\n",
    "            y_pred.append(np.argmin(temp_vec))\n",
    "            \n",
    "        # Needs to be an array    \n",
    "        y_pred = np.array(y_pred)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncc = NCC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Naive Bayesian Classifier (NBC)\n",
    "\n",
    "Implement a Naive Bayesian Classifier (NBC) based on discrete (statistical) values (i.e., counts of examples falling into the different classes and attribute value groups) both for the priors and for the conditional probabilities. Run this on the two SciKitLearn digits data sets. It should also work with the (non-normalised) MNIST_Light set, but it will probably take a (very long) while and not give anything interesting really..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBC:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conditional = dict()\n",
    "        self.priors = dict() # P(class)\n",
    "    \n",
    "    def fit(self, X_train, y_train): # smoothing for fixing the variance , smoothing=1e-2\n",
    "        \n",
    "        labels = set(y_train)\n",
    "        for c in labels:\n",
    "            temp = {}\n",
    "            current_x = X_train[y_train == c]\n",
    "            \n",
    "            # Transpose matrix to get all columns to rows\n",
    "            trans_matrix = current_x.T\n",
    "            pixels = np.unique(current_x)\n",
    "            \n",
    "            # Computes the probability of each pixels occurance in a specific image\n",
    "            for col,row in enumerate(trans_matrix):\n",
    "                unique, counts = np.unique(row, return_counts=True)\n",
    "                temp[col] =  dict(zip(unique, counts/len(current_x)))\n",
    "            self.conditional[c] = temp\n",
    "               \n",
    "            # Computes the mean of each class\n",
    "            self.priors[c] = float(len(y_train[y_train == c])) / len(y_train)\n",
    "    \n",
    "        return None \n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        predicted_classes = []\n",
    "        # p(c)*Prod(p(x_i|c))\n",
    "        for image_pixels in X_test:\n",
    "            temp = []\n",
    "            for c in self.priors.keys():\n",
    "                prob = self.priors[c]\n",
    "                for col,value in enumerate(image_pixels):\n",
    "                    try:\n",
    "                        prob *= self.conditional[c][col][value]\n",
    "                    except:\n",
    "                        prob = 0\n",
    "                temp.append(prob)\n",
    "                \n",
    "            predicted_classes.append(np.argmax(temp))\n",
    "        \n",
    "        y_pred = np.asarray(predicted_classes)\n",
    "  \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc = NBC()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Gaussian Naive Bayesian Classifier (GNB)\n",
    "\n",
    "Implement your own Gaussian Naive Bayesian Classifier (GNB) (assume priors for the classes based on counts and Gaussian distributions for the conditional probabilities) and make sure it works for all three data sets. You will most likely encounter problems due to the edge pixels having value 0.0 in practically ALL images in ALL data sets. A workaround is to add an epsilon to the variance. Why is that still working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNB:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gaussians = dict()\n",
    "        self.priors = dict()\n",
    "    \n",
    "    def fit(self, X_train, y_train, smoothing=1e-2): # smoothing for fixing the variance\n",
    "        \n",
    "        labels = set(y_train)\n",
    "        for c in labels:\n",
    "            current_x = X_train[y_train == c]\n",
    "            # Dictionary with class as key and each key containg its corresponding mean and variance\n",
    "            self.gaussians[c] = {\n",
    "                'mean': current_x.mean(axis=0),\n",
    "                'var': current_x.var(axis=0) + smoothing,\n",
    "            }\n",
    "            # Computes the mean\n",
    "            self.priors[c] = float(len(y_train[y_train == c])) / len(y_train)\n",
    "        \n",
    "        return None # self.gaussians, self.priors\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        # Builds the P matrix of size (number images, number of classes)\n",
    "        row, col = X_test.shape\n",
    "        G = len(self.gaussians)\n",
    "        P = np.zeros((row, G))\n",
    "        \n",
    "        for c, g in iteritems(self.gaussians):\n",
    "            mean, var = g['mean'], g['var']\n",
    "            \n",
    "            # We can choose the digit class using c = argmax_c(logP(x|c)+logP(c))\n",
    "            # mvn.logpdf() since we wants log of the probability density function\n",
    "            P[:,c] = mvn.logpdf(X_test, mean=mean, cov=var) + np.log(self.priors[c])\n",
    "              \n",
    "        return np.argmax(P, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GNB = GNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. SciKitLearn digits\n",
    "\n",
    "SciKitLearn digits: just load it and use it as is. Use 70% of the data for training and 30% for testing. Run all of the four classifiers and compare and discuss the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "num_examples_digits = len(digits.data)\n",
    "num_split_digits = int(0.7*num_examples_digits)\n",
    "train_features_digits = digits.data[:num_split_digits]\n",
    "train_labels_digits = digits.target[:num_split_digits]\n",
    "test_features_digits = digits.data[num_split_digits:]\n",
    "test_labels_digits = digits.target[num_split_digits:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SKLearn GNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        53\n",
      "           1       0.65      0.79      0.71        53\n",
      "           2       0.98      0.81      0.89        53\n",
      "           3       0.92      0.64      0.76        53\n",
      "           4       1.00      0.86      0.92        57\n",
      "           5       0.83      0.93      0.87        56\n",
      "           6       0.96      0.98      0.97        54\n",
      "           7       0.73      0.83      0.78        54\n",
      "           8       0.59      0.71      0.64        52\n",
      "           9       0.82      0.73      0.77        55\n",
      "\n",
      "    accuracy                           0.83       540\n",
      "   macro avg       0.84      0.82      0.83       540\n",
      "weighted avg       0.84      0.83      0.83       540\n",
      "\n",
      "\n",
      "Confusion matrix SKLearn GNB:\n",
      "[[51  0  0  0  0  0  0  0  2  0]\n",
      " [ 0 42  1  0  0  0  0  0  3  7]\n",
      " [ 0  5 43  1  0  0  1  0  1  2]\n",
      " [ 0  3  0 34  0  3  0  2 11  0]\n",
      " [ 1  0  0  0 49  0  0  6  1  0]\n",
      " [ 0  2  0  0  0 52  1  1  0  0]\n",
      " [ 0  1  0  0  0  0 53  0  0  0]\n",
      " [ 0  0  0  0  0  2  0 45  7  0]\n",
      " [ 0 11  0  1  0  1  0  2 37  0]\n",
      " [ 1  1  0  1  0  5  0  6  1 40]]\n"
     ]
    }
   ],
   "source": [
    "gnb.fit(train_features_digits, train_labels_digits)\n",
    "y_pred_digits = gnb.predict(test_features_digits)\n",
    "\n",
    "print(\"Classification report SKLearn GNB:\\n%s\\n\"\n",
    "  % (metrics.classification_report(test_labels_digits, y_pred_digits)))\n",
    "print(\"Confusion matrix SKLearn GNB:\\n%s\" % metrics.confusion_matrix(test_labels_digits, y_pred_digits))\n",
    "\n",
    "# mnist.visualize_wrong_class(y_pred_digits, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest Centroid Classifier (NCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SKLearn GNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        53\n",
      "           1       0.89      0.75      0.82        53\n",
      "           2       0.98      0.85      0.91        53\n",
      "           3       0.84      0.77      0.80        53\n",
      "           4       0.98      0.93      0.95        57\n",
      "           5       0.86      0.89      0.88        56\n",
      "           6       0.98      0.96      0.97        54\n",
      "           7       0.90      0.96      0.93        54\n",
      "           8       0.80      0.75      0.77        52\n",
      "           9       0.65      0.87      0.74        55\n",
      "\n",
      "    accuracy                           0.87       540\n",
      "   macro avg       0.88      0.87      0.88       540\n",
      "weighted avg       0.88      0.87      0.88       540\n",
      "\n",
      "\n",
      "Confusion matrix SKLearn GNB:\n",
      "[[52  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 40  0  0  0  1  0  0  0 12]\n",
      " [ 1  0 45  6  0  0  0  0  0  1]\n",
      " [ 0  1  0 41  0  2  0  3  5  1]\n",
      " [ 1  0  0  0 53  0  0  0  3  0]\n",
      " [ 0  0  0  0  0 50  1  0  0  5]\n",
      " [ 0  2  0  0  0  0 52  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 52  2  0]\n",
      " [ 0  2  1  0  0  1  0  2 39  7]\n",
      " [ 0  0  0  2  0  4  0  1  0 48]]\n"
     ]
    }
   ],
   "source": [
    "ncc.fit(train_features_digits, train_labels_digits)\n",
    "y_pred_digits = ncc.predict(test_features_digits)\n",
    "\n",
    "print(\"Classification report SKLearn GNB:\\n%s\\n\"\n",
    "  % (metrics.classification_report(test_labels_digits, y_pred_digits)))\n",
    "print(\"Confusion matrix SKLearn GNB:\\n%s\" % metrics.confusion_matrix(test_labels_digits, y_pred_digits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayesian Classifier (NBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SKLearn NBC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      1.00      0.39        53\n",
      "           1       0.84      0.51      0.64        53\n",
      "           2       1.00      0.36      0.53        53\n",
      "           3       1.00      0.58      0.74        53\n",
      "           4       0.95      0.65      0.77        57\n",
      "           5       0.88      0.52      0.65        56\n",
      "           6       1.00      0.65      0.79        54\n",
      "           7       0.86      0.59      0.70        54\n",
      "           8       0.72      0.65      0.69        52\n",
      "           9       0.75      0.65      0.70        55\n",
      "\n",
      "    accuracy                           0.62       540\n",
      "   macro avg       0.83      0.62      0.66       540\n",
      "weighted avg       0.83      0.62      0.66       540\n",
      "\n",
      "\n",
      "Confusion matrix SKLearn NBC:\n",
      "[[53  0  0  0  0  0  0  0  0  0]\n",
      " [20 27  0  0  1  0  0  0  3  2]\n",
      " [28  0 19  0  0  0  0  0  5  1]\n",
      " [13  0  0 31  0  3  0  1  3  2]\n",
      " [20  0  0  0 37  0  0  0  0  0]\n",
      " [19  2  0  0  0 29  0  1  1  4]\n",
      " [18  0  0  0  0  0 35  0  1  0]\n",
      " [21  0  0  0  0  0  0 32  0  1]\n",
      " [13  2  0  0  1  0  0  0 34  2]\n",
      " [14  1  0  0  0  1  0  3  0 36]]\n"
     ]
    }
   ],
   "source": [
    "nbc.fit(train_features_digits, train_labels_digits)\n",
    "y_pred_digits = nbc.predict(test_features_digits)\n",
    "\n",
    "print(\"Classification report SKLearn NBC:\\n%s\\n\"\n",
    "  % (metrics.classification_report(test_labels_digits, y_pred_digits)))\n",
    "print(\"Confusion matrix SKLearn NBC:\\n%s\" % metrics.confusion_matrix(test_labels_digits, y_pred_digits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayesian Classifier (GNB) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test (540, 64)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "(540,)\n",
      "P [4 0 5 3 6 9 6 2 7 5 4 4 7 2 8 2 3 5 7 9 5 4 8 8 4 9 0 8 9 8 0 9 2 3 4 5 6\n",
      " 8 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7\n",
      " 7 3 5 1 0 0 2 1 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 6 2 8 9 0 0 1\n",
      " 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 8 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5\n",
      " 7 9 5 4 8 8 4 9 0 8 0 9 2 3 4 5 6 7 9 9 0 9 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6\n",
      " 7 8 9 0 9 5 5 6 5 0 9 8 3 8 4 9 7 7 3 5 9 0 0 2 2 7 8 2 0 9 2 6 3 3 7 3 3\n",
      " 4 6 6 6 4 9 9 5 0 5 5 2 9 2 0 0 9 7 6 3 2 9 7 4 6 3 1 3 9 9 7 6 8 4 3 9 4\n",
      " 0 5 3 6 9 6 9 7 5 4 4 7 2 9 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1 2 3 4 5 9 2\n",
      " 1 9 0 8 2 3 4 5 6 9 0 1 2 3 4 5 1 7 5 7 4 9 5 5 6 5 0 7 7 5 8 4 1 7 8 8 5\n",
      " 1 4 0 1 2 8 8 1 0 1 2 6 8 8 8 7 8 7 6 6 6 7 9 1 5 4 9 1 2 8 0 1 7 6 3 2 1\n",
      " 5 7 6 3 1 3 7 1 8 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 2 5 5 3 5 8 4 5 0\n",
      " 8 7 7 0 8 2 8 4 5 6 7 8 9 0 1 2 8 4 5 6 7 8 9 0 1 2 5 4 5 6 7 8 9 0 9 5 5\n",
      " 6 5 0 9 8 9 8 4 1 7 7 7 5 1 0 0 2 2 7 8 2 0 1 2 6 8 8 4 9 8 4 6 6 6 4 9 1\n",
      " 5 0 9 5 2 8 2 0 0 8 7 6 3 2 1 7 4 6 3 8 3 9 1 7 6 8 4 5 1 4 0 5 3 6 9 6 1\n",
      " 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 1 4 9 0 8 9 8]\n",
      "Classification report SKLearn GNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        53\n",
      "           1       0.84      0.68      0.75        53\n",
      "           2       0.96      0.91      0.93        53\n",
      "           3       0.93      0.74      0.82        53\n",
      "           4       0.93      0.93      0.93        57\n",
      "           5       0.87      0.95      0.91        56\n",
      "           6       0.98      0.96      0.97        54\n",
      "           7       0.80      0.83      0.82        54\n",
      "           8       0.69      0.85      0.76        52\n",
      "           9       0.72      0.84      0.77        55\n",
      "\n",
      "    accuracy                           0.86       540\n",
      "   macro avg       0.87      0.86      0.86       540\n",
      "weighted avg       0.87      0.86      0.86       540\n",
      "\n",
      "\n",
      "Confusion matrix SKLearn GNB:\n",
      "[[50  0  0  0  3  0  0  0  0  0]\n",
      " [ 0 36  1  0  0  0  0  0  4 12]\n",
      " [ 0  3 48  1  0  0  0  0  0  1]\n",
      " [ 0  0  0 39  0  2  0  2  9  1]\n",
      " [ 0  0  0  0 53  0  0  3  1  0]\n",
      " [ 0  1  0  0  0 53  1  0  1  0]\n",
      " [ 0  1  0  0  0  0 52  0  0  1]\n",
      " [ 0  0  1  0  1  2  0 45  5  0]\n",
      " [ 0  2  0  0  0  1  0  2 44  3]\n",
      " [ 0  0  0  2  0  3  0  4  0 46]]\n"
     ]
    }
   ],
   "source": [
    "GNB.fit(train_features_digits, train_labels_digits)\n",
    "y_pred_digits = GNB.predict(test_features_digits)\n",
    "\n",
    "print(\"Classification report SKLearn GNB:\\n%s\\n\"\n",
    "  % (metrics.classification_report(test_labels_digits, y_pred_digits)))\n",
    "print(\"Confusion matrix SKLearn GNB:\\n%s\" % metrics.confusion_matrix(test_labels_digits, y_pred_digits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. SciKitLearn digits summarised\n",
    "\n",
    "SciKitLearn digits summarised: reduce the data set to only contain three values for the attributes, e.g., 0 for 'dark', 1 for 'grey' and 2 for 'light', with dark, grey and light corresponding to the values suggested in lab 2 (decision trees). Split again into 70% training and 30% test data.\n",
    "Run all four classifiers on this set and compare the results. Why are they so different from those for the original data in particular for the NBC? Why do they decrease in accuracy for the GNB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp(vec):\n",
    "    \"\"\"Sets the pixel values to either 0, 1 or 2, that is, dark, grey or light\"\"\"\n",
    "    vec_new = np.empty([len(vec)]) \n",
    "    if len(vec) == 1: vec = [vec]\n",
    "    for i,x in enumerate(vec):\n",
    "        if x<=5: vec_new[i] = 0\n",
    "        elif x<=10: vec_new[i] = 1\n",
    "        else: vec_new[i] = 2\n",
    "    return vec_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_comp = np.asarray([comp(i) for i in train_features_digits])\n",
    "train_labels_comp = np.asarray(train_labels_digits)\n",
    "test_features_comp = np.asarray([comp(i) for i in test_features_digits])\n",
    "test_labels_comp = np.asarray((test_labels_digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SKLearn GNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90        53\n",
      "           1       0.93      0.26      0.41        53\n",
      "           2       1.00      0.74      0.85        53\n",
      "           3       0.97      0.66      0.79        53\n",
      "           4       0.94      0.84      0.89        57\n",
      "           5       0.86      0.57      0.69        56\n",
      "           6       0.93      0.98      0.95        54\n",
      "           7       0.64      0.89      0.74        54\n",
      "           8       0.41      0.88      0.56        52\n",
      "           9       0.63      0.76      0.69        55\n",
      "\n",
      "    accuracy                           0.75       540\n",
      "   macro avg       0.82      0.75      0.75       540\n",
      "weighted avg       0.83      0.75      0.75       540\n",
      "\n",
      "\n",
      "Confusion matrix SKLearn GNB:\n",
      "[[47  0  0  0  2  1  0  0  3  0]\n",
      " [ 0 14  0  0  1  1  0  6 19 12]\n",
      " [ 0  0 39  0  0  0  0  0 12  2]\n",
      " [ 0  0  0 35  0  0  0  4 12  2]\n",
      " [ 1  0  0  0 48  0  3  4  1  0]\n",
      " [ 1  0  0  0  0 32  1  4 12  6]\n",
      " [ 0  0  0  0  0  0 53  0  0  1]\n",
      " [ 0  0  0  0  0  2  0 48  4  0]\n",
      " [ 0  1  0  0  0  0  0  3 46  2]\n",
      " [ 2  0  0  1  0  1  0  6  3 42]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 540 is out of bounds for axis 0 with size 540",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-348285b38b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Confusion matrix SKLearn GNB:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels_comp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_comp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize_wrong_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_comp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/AppliedML/A5/Handout_NaiveBayes/MNIST.py\u001b[0m in \u001b[0;36mvisualize_wrong_class\u001b[0;34m(self, pred, examples_per_class)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_wrong_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples_per_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mexamples_per_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples_per_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/AppliedML/A5/Handout_NaiveBayes/MNIST.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_wrong_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples_per_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mexamples_per_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples_per_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 540 is out of bounds for axis 0 with size 540"
     ]
    }
   ],
   "source": [
    "gnb.fit(train_features_comp, train_labels_comp)\n",
    "y_pred_comp = gnb.predict(test_features_comp)\n",
    "\n",
    "print(\"Classification report SKLearn digits summarize GNB:\\n%s\\n\"\n",
    "  % (metrics.classification_report(test_labels_comp, y_pred_comp)))\n",
    "print(\"Confusion matrix SKLearn digits summarize GNB:\\n%s\" % metrics.confusion_matrix(test_labels_comp, y_pred_comp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest Centroid Classifier (NCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SKLearn GNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        53\n",
      "           1       0.85      0.64      0.73        53\n",
      "           2       0.93      0.81      0.87        53\n",
      "           3       0.77      0.77      0.77        53\n",
      "           4       0.96      0.93      0.95        57\n",
      "           5       0.83      0.88      0.85        56\n",
      "           6       0.96      0.96      0.96        54\n",
      "           7       0.91      0.93      0.92        54\n",
      "           8       0.77      0.83      0.80        52\n",
      "           9       0.67      0.84      0.74        55\n",
      "\n",
      "    accuracy                           0.86       540\n",
      "   macro avg       0.86      0.85      0.86       540\n",
      "weighted avg       0.86      0.86      0.86       540\n",
      "\n",
      "\n",
      "Confusion matrix SKLearn GNB:\n",
      "[[51  0  0  0  1  1  0  0  0  0]\n",
      " [ 0 34  0  1  0  1  0  0  4 13]\n",
      " [ 1  1 43  7  0  0  0  0  0  1]\n",
      " [ 0  1  1 41  0  2  0  3  4  1]\n",
      " [ 1  0  0  0 53  0  0  0  3  0]\n",
      " [ 0  1  0  0  0 49  2  0  0  4]\n",
      " [ 0  2  0  0  0  0 52  0  0  0]\n",
      " [ 0  0  2  0  0  0  0 50  2  0]\n",
      " [ 0  1  0  0  1  2  0  1 43  4]\n",
      " [ 0  0  0  4  0  4  0  1  0 46]]\n"
     ]
    }
   ],
   "source": [
    "ncc.fit(train_features_comp, train_labels_comp)\n",
    "y_pred_comp = ncc.predict(test_features_comp)\n",
    "\n",
    "print(\"Classification report SKLearn digits summarize NCC:\\n%s\\n\"\n",
    "  % (metrics.classification_report(test_labels_comp, y_pred_comp)))\n",
    "print(\"Confusion matrix SKLearn digits summarize NCC:\\n%s\" % metrics.confusion_matrix(test_labels_comp, y_pred_comp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayesian Classifier (NBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SKLearn digits summarize NBC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.79      0.82        53\n",
      "           1       0.85      0.74      0.79        53\n",
      "           2       0.96      0.89      0.92        53\n",
      "           3       0.83      0.74      0.78        53\n",
      "           4       0.95      0.95      0.95        57\n",
      "           5       0.79      0.88      0.83        56\n",
      "           6       0.98      0.87      0.92        54\n",
      "           7       0.83      0.83      0.83        54\n",
      "           8       0.72      0.83      0.77        52\n",
      "           9       0.67      0.82      0.74        55\n",
      "\n",
      "    accuracy                           0.83       540\n",
      "   macro avg       0.84      0.83      0.83       540\n",
      "weighted avg       0.84      0.83      0.84       540\n",
      "\n",
      "\n",
      "Confusion matrix SKLearn digits summarize NBC:\n",
      "[[42  0  0  1  3  4  0  1  2  0]\n",
      " [ 0 39  0  0  0  1  0  0  1 12]\n",
      " [ 2  0 47  3  0  1  0  0  0  0]\n",
      " [ 0  1  1 39  0  1  0  3  6  2]\n",
      " [ 0  0  0  0 54  0  0  1  2  0]\n",
      " [ 1  2  0  0  0 49  1  0  1  2]\n",
      " [ 2  1  0  0  0  2 47  0  0  2]\n",
      " [ 3  0  0  0  0  1  0 45  5  0]\n",
      " [ 0  2  1  0  0  1  0  1 43  4]\n",
      " [ 0  1  0  4  0  2  0  3  0 45]]\n"
     ]
    }
   ],
   "source": [
    "nbc.fit(train_features_comp, train_labels_comp)\n",
    "y_pred_comp = nbc.predict(test_features_comp)\n",
    "\n",
    "print(\"Classification report SKLearn digits summarize NBC:\\n%s\\n\"\n",
    "  % (metrics.classification_report(test_labels_comp, y_pred_comp)))\n",
    "print(\"Confusion matrix SKLearn digits summarize NBC:\\n%s\" % metrics.confusion_matrix(test_labels_comp, y_pred_comp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayesian Classifier (GNB) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SKLearn GNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.93        53\n",
      "           1       0.92      0.66      0.77        53\n",
      "           2       0.96      0.96      0.96        53\n",
      "           3       0.85      0.74      0.79        53\n",
      "           4       0.92      0.95      0.93        57\n",
      "           5       0.89      0.88      0.88        56\n",
      "           6       0.98      0.96      0.97        54\n",
      "           7       0.82      0.87      0.85        54\n",
      "           8       0.66      0.87      0.75        52\n",
      "           9       0.69      0.76      0.72        55\n",
      "\n",
      "    accuracy                           0.86       540\n",
      "   macro avg       0.87      0.85      0.86       540\n",
      "weighted avg       0.87      0.86      0.86       540\n",
      "\n",
      "\n",
      "Confusion matrix SKLearn GNB:\n",
      "[[48  0  0  0  4  0  0  0  1  0]\n",
      " [ 0 35  0  0  1  1  0  0  4 12]\n",
      " [ 1  0 51  1  0  0  0  0  0  0]\n",
      " [ 0  0  1 39  0  1  0  3  8  1]\n",
      " [ 0  0  0  0 54  0  0  2  1  0]\n",
      " [ 0  1  0  0  0 49  1  0  3  2]\n",
      " [ 0  1  0  0  0  0 52  0  0  1]\n",
      " [ 0  0  0  0  0  1  0 47  6  0]\n",
      " [ 0  1  1  0  0  1  0  1 45  3]\n",
      " [ 1  0  0  6  0  2  0  4  0 42]]\n"
     ]
    }
   ],
   "source": [
    "GNB.fit(train_features_comp, train_labels_comp)\n",
    "y_pred_comp = GNB.predict(test_features_comp)\n",
    "\n",
    "print(\"Classification report SKLearn digits summarize GNB:\\n%s\\n\"\n",
    "  % (metrics.classification_report(test_labels_comp, y_pred_comp)))\n",
    "print(\"Confusion matrix SKLearn digits summarize GNB:\\n%s\" % metrics.confusion_matrix(test_labels_comp, y_pred_comp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. MNIST_Light\n",
    "\n",
    "MNIST_Light: load the MINST_Light data set (see above), inspect the contents (data and specifically the target) carefully and split the set into 70% training and 30% test data if you do not use the code in the handout (based on MNIST.py) as is. If you use the provided code to retrieve the data set, the three-dimensional (20x20-pixels per image) arrays will have been reshaped to be of the same two dimensions as given in the digits set (one flattened array per image) and the values for the attributes (pixels) will have been normalised from [0 ... 255] to [0.0 ... 1.0]. If you do not normalise them, you can observe a problem with the GNB. What is this problem and why is it solved by normalisation of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST.MNISTData('MNIST_Light/*/*.png')\n",
    "train_features_mnist, test_features_mnist, train_labels_mnist, test_labels_mnist = mnist.get_data()\n",
    "# mnist.visualize_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SKLearn GNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.94      0.69       164\n",
      "           1       0.71      0.94      0.81       152\n",
      "           2       0.83      0.50      0.62       155\n",
      "           3       0.83      0.53      0.65       154\n",
      "           4       0.75      0.31      0.44       143\n",
      "           5       0.67      0.16      0.25       141\n",
      "           6       0.81      0.85      0.83       143\n",
      "           7       0.83      0.82      0.83       158\n",
      "           8       0.41      0.64      0.50       132\n",
      "           9       0.60      0.84      0.70       158\n",
      "\n",
      "    accuracy                           0.66      1500\n",
      "   macro avg       0.70      0.65      0.63      1500\n",
      "weighted avg       0.70      0.66      0.64      1500\n",
      "\n",
      "\n",
      "Confusion matrix SKLearn GNB:\n",
      "[[154   0   6   0   1   1   0   0   1   1]\n",
      " [  1 143   1   0   0   1   0   1   3   2]\n",
      " [ 11   6  77  10   2   1  19   1  27   1]\n",
      " [ 32  11   5  82   0   0   2   3  12   7]\n",
      " [ 10   1   2   2  45   2   6   9  24  42]\n",
      " [ 55   7   0   2   2  22   1   0  47   5]\n",
      " [ 10   7   0   0   0   2 122   0   2   0]\n",
      " [  2   1   1   2   3   0   0 130   3  16]\n",
      " [  5  23   1   1   0   4   1   0  84  13]\n",
      " [  3   1   0   0   7   0   0  12   2 133]]\n"
     ]
    }
   ],
   "source": [
    "gnb.fit(train_features_mnist, train_labels_mnist)\n",
    "y_pred_mnist = gnb.predict(test_features_mnist)\n",
    "\n",
    "print(\"Classification report MNIST_Light GNB:\\n%s\\n\"\n",
    "  % (metrics.classification_report(test_labels_mnist, y_pred_mnist)))\n",
    "print(\"Confusion matrix MNIST_Light GNB:\\n%s\" % metrics.confusion_matrix(test_labels_mnist, y_pred_mnist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest Centroid Classifier (NCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SKLearn GNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       164\n",
      "           1       0.71      0.97      0.82       152\n",
      "           2       0.84      0.73      0.78       155\n",
      "           3       0.74      0.76      0.75       154\n",
      "           4       0.75      0.76      0.75       143\n",
      "           5       0.72      0.69      0.70       141\n",
      "           6       0.90      0.86      0.88       143\n",
      "           7       0.95      0.80      0.87       158\n",
      "           8       0.79      0.72      0.75       132\n",
      "           9       0.76      0.80      0.78       158\n",
      "\n",
      "    accuracy                           0.80      1500\n",
      "   macro avg       0.81      0.80      0.80      1500\n",
      "weighted avg       0.81      0.80      0.80      1500\n",
      "\n",
      "\n",
      "Confusion matrix SKLearn GNB:\n",
      "[[150   0   2   0   0   6   3   1   2   0]\n",
      " [  0 148   0   0   0   2   0   0   2   0]\n",
      " [  0  15 113   8   2   3   3   1   8   2]\n",
      " [  1   5   8 117   1   7   1   2   8   4]\n",
      " [  1   4   2   0 108   0   3   0   1  24]\n",
      " [  3   9   0  24   4  97   2   0   1   1]\n",
      " [  3   6   2   0   4   5 123   0   0   0]\n",
      " [  1  14   2   0   6   1   0 127   1   6]\n",
      " [  3   6   4   8   0  12   1   0  95   3]\n",
      " [  3   0   1   1  19   2   1   3   2 126]]\n"
     ]
    }
   ],
   "source": [
    "ncc.fit(train_features_mnist, train_labels_mnist)\n",
    "y_pred_mnist = ncc.predict(test_features_mnist)\n",
    "\n",
    "print(\"Classification report MNIST_Light NCC:\\n%s\\n\"\n",
    "  % (metrics.classification_report(test_labels_mnist, y_pred_mnist)))\n",
    "print(\"Confusion matrix MNIST_Light NCC:\\n%s\" % metrics.confusion_matrix(test_labels_mnist, y_pred_mnist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayesian Classifier (GNB) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report SKLearn GNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       164\n",
      "           1       0.76      0.97      0.85       152\n",
      "           2       0.83      0.70      0.76       155\n",
      "           3       0.76      0.74      0.75       154\n",
      "           4       0.81      0.64      0.71       143\n",
      "           5       0.93      0.67      0.78       141\n",
      "           6       0.84      0.94      0.89       143\n",
      "           7       0.96      0.81      0.88       158\n",
      "           8       0.74      0.75      0.74       132\n",
      "           9       0.66      0.88      0.75       158\n",
      "\n",
      "    accuracy                           0.81      1500\n",
      "   macro avg       0.82      0.80      0.80      1500\n",
      "weighted avg       0.82      0.81      0.81      1500\n",
      "\n",
      "\n",
      "Confusion matrix SKLearn GNB:\n",
      "[[156   0   1   0   0   1   2   0   3   1]\n",
      " [  0 148   0   0   0   1   1   0   2   0]\n",
      " [  1   6 109  11   1   1  13   1  12   0]\n",
      " [  0   6  15 114   1   0   2   2   9   5]\n",
      " [  0   1   2   0  91   0   4   0   3  42]\n",
      " [  8   5   0  21   2  94   1   1   4   5]\n",
      " [  3   4   0   0   0   1 134   0   1   0]\n",
      " [  0  11   3   0   6   0   0 128   0  10]\n",
      " [  1  12   2   3   1   2   2   0  99  10]\n",
      " [  2   2   0   1  11   1   0   1   1 139]]\n"
     ]
    }
   ],
   "source": [
    "GNB.fit(train_features_mnist, train_labels_mnist)\n",
    "y_pred_mnist = GNB.predict(test_features_mnist)\n",
    "\n",
    "print(\"Classification report MNIST_Light GNB:\\n%s\\n\"\n",
    "  % (metrics.classification_report(test_labels_mnist, y_pred_mnist)))\n",
    "print(\"Confusion matrix MNIST_Light GNB:\\n%s\" % metrics.confusion_matrix(test_labels_mnist, y_pred_mnist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
